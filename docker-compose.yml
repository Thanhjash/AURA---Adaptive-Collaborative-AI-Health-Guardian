services:
  ai_server:
    image: vgxtj/aura-ai-server:latest
    build:
      context: .
      dockerfile: ./services/ai_server/Dockerfile
    ports:
      - "9000:9000"
    env_file:
      - .env
    environment:
      - HF_HOME=/app/.hf_cache
    healthcheck:
      test: ["CMD", "python", "-c", "import requests,sys; r=requests.get('http://localhost:9000/health',timeout=10); sys.exit(0 if r.json().get('model_loaded') else 1)"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 300s  # MedGemma needs time to download/load
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '6'
    networks: [aura_network]
    restart: unless-stopped

  aura_main:
    image: vgxtj/aura-main:latest
    build:
      context: .
      dockerfile: services/aura_main/Dockerfile
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - ENABLE_EXPERT_COUNCIL=false  # Set to true to enable
    volumes:
      - ./config/secrets:/app/config/secrets:ro
    depends_on:
      ai_server: { condition: service_healthy }
    healthcheck:
      test: ["CMD", "python", "-c", "import requests,sys; r=requests.get('http://localhost:8000/health',timeout=5); sys.exit(0 if r.json().get('status')=='healthy' else 1)"]
      interval: 20s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks: [aura_network]
    restart: unless-stopped

  # OPTIONAL: Keep for Expert Council or future expansion
  # Uncomment if needed
  # ecg_interpreter:
  #   image: vgxtj/aura-ecg-interpreter:latest
  #   build:
  #     context: .
  #     dockerfile: services/ecg_interpreter/Dockerfile
  #   ports:
  #     - "8001:8001"
  #   env_file:
  #     - .env
  #   networks: [aura_network]

  # radiology_vqa:
  #   image: vgxtj/aura-radiology-vqa:latest
  #   build:
  #     context: .
  #     dockerfile: services/radiology_vqa/Dockerfile
  #   ports:
  #     - "8002:8002"
  #   env_file:
  #     - .env
  #   networks: [aura_network]

  # mental_wellness:
  #   image: vgxtj/aura-mental-wellness:latest
  #   build:
  #     context: .
  #     dockerfile: services/mental_wellness/Dockerfile
  #   ports:
  #     - "8003:8003"
  #   env_file:
  #     - .env
  #   networks: [aura_network]

networks:
  aura_network:
    driver: bridge